{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IceOnDunes/speech-recognition/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "F6hX918kaDn_"
      },
      "outputs": [],
      "source": [
        "from CTCModel import CTCModel as CTCModel\n",
        "from Model import model\n",
        "from fcts import update_dataframe,decode\n",
        "import matplotlib.pyplot as plt\n",
        "from Data_generator import DataGenerator\n",
        "from tensorflow.keras.optimizers  import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "hsyzKE7nHak_"
      },
      "outputs": [],
      "source": [
        "csv_path='list_test_TIMIT.csv'\n",
        "test_path = pd.read_csv(csv_path, header=None)\n",
        "test_path=list(test_path[0])\n",
        "test_generator2 = DataGenerator(test_path, **data_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ1F3KQqaTnA",
        "outputId": "d13431eb-7e38-48c1-c856-2f2963a160c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, None, 129)\n",
            "(None, None, 1000)\n",
            "(None, None, 1000)\n",
            "(None, None, 1000)\n",
            "(None, None, 1000)\n",
            "(None, None, 1000)\n",
            "(None, None, 28)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Model training\n",
        "\n",
        "\"\"\"\n",
        "model = model()\n",
        "model.compile(optimizer=Adam(lr=0.0001))\n",
        "model.load_weights('saved-model_6-19-60.64.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqrGvhDQHalA",
        "outputId": "fb529940-97a5-4543-cf19-d8162fb11382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6a41a91170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "( 1 )\n",
            "predicted :  she  had  your  dark  sui  in  greasy  wash  water  a  year\n",
            "Ground truth :  she  had  your  dark  suit  in  greasy  wash  water  all  year\n",
            "\n",
            "\n",
            "( 2 )\n",
            "predicted :  dnt  ask  me  to  cary  an  oily  rag  like  that\n",
            "Ground truth :  dont  ask  me  to  carry  an  oily  rag  like  that\n",
            "\n",
            "\n",
            "( 3 )\n",
            "predicted :  the  digraane  axes  wonley  ater  mouh  study\n",
            "Ground truth :  that  diagram  makes  sense  only  after  much  study\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Prediction\n",
        "\"\"\"\n",
        "print(\"Prediction\")\n",
        "pred = model.predict_generator(test_generator2,steps=1)\n",
        "\"\"\"\n",
        "Decoding prediction results\n",
        "\"\"\"\n",
        "y_pred = []\n",
        "y_true = []\n",
        "for i in range (len(pred[0])):\n",
        "    y_pred.append(decode(pred[0][i]))\n",
        "    y_true.append(decode(pred[1][i]))\n",
        "    \n",
        "    \n",
        "\"\"\"\n",
        "Predictions Visualisation\n",
        "\"\"\"\n",
        "\n",
        "for i in range (len(test_path)):\n",
        "    print(\"(\",i+1,\")\")\n",
        "    y_pred=decode(pred[0][i])\n",
        "    print(\"predicted : \",y_pred)\n",
        "\n",
        "    y_true=decode(pred[1][i])\n",
        "\n",
        "    print(\"Ground truth : \",y_true)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "yDlzbEEoHak7"
      },
      "outputs": [],
      "source": [
        "csv_path='list_test_VCTK.csv'\n",
        "\n",
        "\n",
        "test_path = pd.read_csv(csv_path, header=None)\n",
        "test_path=list(test_path[0])\n",
        "data_params = {'window_len' : 128,\n",
        "               'hop_len' : 127,\n",
        "               'nfft' : 256,\n",
        "               'batch_size' : len(test_path),\n",
        "               'shuffle' : False\n",
        "               }\n",
        "test_generator = DataGenerator(test_path, **data_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdMVxTM8aYoQ",
        "outputId": "07303ace-f2a9-4439-995b-0eafb21cb374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6a41a91170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Prediction\n",
        "\"\"\"\n",
        "print(\"Prediction\")\n",
        "pred = model.predict_generator(test_generator,steps=1)\n",
        "\"\"\"\n",
        "Decoding prediction results\n",
        "\"\"\"\n",
        "y_pred = []\n",
        "y_true = []\n",
        "for i in range (len(pred[0])):\n",
        "    y_pred.append(decode(pred[0][i]))\n",
        "    y_true.append(decode(pred[1][i]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaK8YVorafYo",
        "outputId": "515f3f50-e80e-4eaf-9ffb-e9a97e0b6350",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "( 1 )\n",
            "predicted :  ietug  orcihalserns  idluin\n",
            "Ground truth :  they  had  four  children  together\n",
            "\n",
            "\n",
            "( 2 )\n",
            "predicted :  igoaveo  wors  beroo\n",
            "Ground truth :  it  was  even  worse  than  at  home\n",
            "\n",
            "\n",
            "( 3 )\n",
            "predicted :  las  foubay  o  beoooohomoohpoo\n",
            "Ground truth :  ask  her  to  bring  these  things  with  her  from  the  store\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Predictions Visualisation\n",
        "\"\"\"\n",
        "\n",
        "for i in range (len(test_path)):\n",
        "    print(\"(\",i+1,\")\")\n",
        "    y_pred=decode(pred[0][i])\n",
        "    print(\"predicted : \",y_pred)\n",
        "\n",
        "    y_true=decode(pred[1][i])\n",
        "\n",
        "    print(\"Ground truth : \",y_true)\n",
        "    print('\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Main.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
